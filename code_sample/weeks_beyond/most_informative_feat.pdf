#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Classification is a supervised learning task, which is one of the central tasks
in machine learning. In this episode we will train several types of supervised
learning algorithms to classify a set of documents according to some classes or
labels (e.g., genre or author features).
"""
__author__= "KLN"

from __future__ import division
import pandas as pd
import numpy as np
import os

from unidecode import unidecode

### data preparation ###
root = os.path.expanduser('~/Documents/data/fake_real_news_dataset-master')
filepath = os.path.join(root,'fake_or_real_news.dat')
data = pd.read_csv(filepath)


data.head()
print type(data.text[0])

print 'fake news n1 = ', sum(data['label']=='FAKE')
print 'real news n2 = ',sum(data['label']=='REAL')

# split data
ratio = .8
mask = np.random.rand(len(data)) <= ratio
data_train = data[mask]
data_test = data[~mask]

train_X = data_train['text'].values
train_y = data_train['label'].values
test_X = data_test['text'].values
test_y = data_test['label'].values

# training
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve
from sklearn.naive_bayes import MultinomialNB


pl = Pipeline([('vectorizer', CountVectorizer()), ('nb_classifier', MultinomialNB())])
pl.fit(train_X, train_y)
pred = pl.predict(test_X)

# validation
confmat = confusion_matrix(test_y, pred)
perf_acc = accuracy_score(test_y, pred)
perf_f1 = f1_score(test_y, pred, pos_label = 'FAKE')


# try your own
pl.predict([data['text'][2]])
pl.predict(['this is my true story'])


########## most informative features
from sklearn.feature_extraction.text import CountVectorizer
# features
vectorizer = CountVectorizer()# instantiate vectorizer
train_feat = vectorizer.fit_transform(train_X)
# classifier
nb_classifier = MultinomialNB()# instantiate classifier
nb_classifier.fit(train_feat, train_y)

n = 10
feat_names = vectorizer.get_feature_names()
coef_feat_names = sorted(zip(nb_classifier.coef_[0],feat_names))
n_most_info = zip(coef_feat_names[:n], coef_feat_names[:-(n + 1):-1])
n_most_info

coefs_with_fns = sorted(zip(nb_classifier.coef_[0], feat_names))

def show_most_informative_features(vectorizer, clf, n=20):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))
    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])
    for (coef_1, fn_1), (coef_2, fn_2) in top:
        print "\t%.4f\t%-15s\t\t%.4f\t%-15s" % (coef_1, fn_1, coef_2, fn_2)


show_most_informative_features(vectorizer, nb_classifier, 100)
